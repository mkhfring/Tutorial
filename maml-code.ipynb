{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkhfring/Tutorial/blob/master/maml-code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzOseH8Gq2RV"
      },
      "source": [
        "!git clone https://github.com/mkhfring/Tutorial.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tElX9YAQrzLJ"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install datasets\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "train_data = datasets.load_from_disk('Tutorial/train_clone')"
      ],
      "metadata": {
        "id": "v0PH9LiYO48i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MA31vCVDncr"
      },
      "source": [
        "import datasets\n",
        "\n",
        "test_data = datasets.load_dataset('json', data_files={'test': 'Tutorial/test_clone.jsonl'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "id": "TijwVPBQoRPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dat['test'][0]\n"
      ],
      "metadata": {
        "id": "wg0Huxajv4lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0]"
      ],
      "metadata": {
        "id": "f8sgZplVw1sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el1AcIwa5PC5"
      },
      "source": [
        "train = train_data\n",
        "import pandas as pd\n",
        "train_df = pd.DataFrame(train)\n",
        "text = train_df[\"func1\"]+ train_df[\"func2\"]\n",
        "train_df['text'] = text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-g_7KciTFZlh"
      },
      "source": [
        "test = test_dat[\"test\"]\n",
        "import pandas as pd\n",
        "test_df = pd.DataFrame(test)\n",
        "text = test_df[\"func1\"]+ test_df[\"func2\"]\n",
        "test_df['text'] = text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNJJH6P1Rab8"
      },
      "source": [
        "train_df['label'] = train_df[\"label\"].replace({True:\"T\", False:\"F\"})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "ExCYtO_nTseq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogwqKedSG8Ho"
      },
      "source": [
        "test_df['label'] = test_df[\"label\"].replace({True:\"T\", False:\"F\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQqJ7cNiHnnh"
      },
      "source": [
        "test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exaFRYPWqzKl"
      },
      "source": [
        "## Creating meta learning tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdjvWFpQqzKl"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import collections\n",
        "import random\n",
        "import json, pickle\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "LABEL_MAP  = {'positive':0, 'negative':1, 0:'positive', 1:'negative'}\n",
        "LABEL_MAP  = { 'T':0, 'F':1, 0:'T', 1:'F'}\n",
        "class MetaTask(Dataset):\n",
        "\n",
        "    def __init__(self, examples, num_task, k_support, k_query, tokenizer):\n",
        "        \"\"\"\n",
        "        :param samples: list of samples\n",
        "        :param num_task: number of training tasks.\n",
        "        :param k_support: number of support sample per task\n",
        "        :param k_query: number of query sample per task\n",
        "        \"\"\"\n",
        "        self.examples = examples\n",
        "        random.shuffle(self.examples)\n",
        "\n",
        "        self.num_task = num_task\n",
        "        self.k_support = k_support\n",
        "        self.k_query = k_query\n",
        "        self.tokenizer = tokenizer\n",
        "        # This part has been changed for the perpous of training with code\n",
        "        # self.max_seq_length = 128\n",
        "        self.max_seq_length = 512\n",
        "\n",
        "        self.create_batch(self.num_task)\n",
        "\n",
        "    def create_batch(self, num_task):\n",
        "        self.supports = []  # support set\n",
        "        self.queries = []  # query set\n",
        "\n",
        "        for b in range(num_task):  # for each task\n",
        "            # 1.select domain randomly\n",
        "            #domain = random.choice(self.examples)['domain']\n",
        "            #domainExamples = [e for e in self.examples if e['domain'] == domain]\n",
        "            domainExamples = [e for e in self.examples]\n",
        "\n",
        "            # 1.select k_support + k_query examples from domain randomly\n",
        "            selected_examples = random.sample(domainExamples,self.k_support + self.k_query)\n",
        "            random.shuffle(selected_examples)\n",
        "            exam_train = selected_examples[:self.k_support]\n",
        "            exam_test  = selected_examples[self.k_support:]\n",
        "\n",
        "            self.supports.append(exam_train)\n",
        "            self.queries.append(exam_test)\n",
        "\n",
        "    def create_feature_set(self,examples):\n",
        "        all_input_ids      = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
        "        all_attention_mask = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
        "        all_segment_ids    = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
        "        all_label_ids      = torch.empty(len(examples), dtype = torch.long)\n",
        "\n",
        "        for id_,example in enumerate(examples):\n",
        "          # I have changed this part of the code\n",
        "          #  input_ids = tokenizer.encode(example['text'])\n",
        "            input_ids = tokenizer.encode(example['text'], max_length=128, truncation=True)\n",
        "\n",
        "            attention_mask = [1] * len(input_ids)\n",
        "            segment_ids    = [0] * len(input_ids)\n",
        "\n",
        "            while len(input_ids) < self.max_seq_length:\n",
        "                input_ids.append(0)\n",
        "                attention_mask.append(0)\n",
        "                segment_ids.append(0)\n",
        "\n",
        "            #print(\"Check point for lables\")\n",
        "\n",
        "            label_id = LABEL_MAP[example['label']]\n",
        "            all_input_ids[id_] = torch.Tensor(input_ids).to(torch.long)\n",
        "            all_attention_mask[id_] = torch.Tensor(attention_mask).to(torch.long)\n",
        "            all_segment_ids[id_] = torch.Tensor(segment_ids).to(torch.long)\n",
        "            all_label_ids[id_] = torch.Tensor([label_id]).to(torch.long)\n",
        "\n",
        "\n",
        "        tensor_set = TensorDataset(all_input_ids, all_attention_mask, all_segment_ids, all_label_ids)\n",
        "        return tensor_set\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        support_set = self.create_feature_set(self.supports[index])\n",
        "        query_set   = self.create_feature_set(self.queries[index])\n",
        "        return support_set, query_set\n",
        "\n",
        "    def __len__(self):\n",
        "        # as we have built up to batchsz of sets, you can sample some small batch size of sets.\n",
        "        return self.num_task"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18dXHS7aqzKm"
      },
      "source": [
        "## Split meta training and meta testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jow_B3WIqzKn"
      },
      "source": [
        "\n",
        "train_examples = train_df.to_dict('records')\n",
        "test_examples = test_df.to_dict('records')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_examples)"
      ],
      "metadata": {
        "id": "W0B-B82KkhI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obTAC-EDIAN_"
      },
      "source": [
        "type(test_examples)\n",
        "random.shuffle(test_examples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXVTMz6JqzKo"
      },
      "source": [
        "import torch\n",
        "from transformers import BertModel, BertTokenizer, AutoTokenizer, RobertaConfig, RobertaTokenizer, RobertaModel\n",
        "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "train = MetaTask(train_examples, num_task = 10, k_support=100, k_query=30, tokenizer = tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7HOIIR4qzKo"
      },
      "source": [
        "#Take a glance at the first two samples from support set of 1st meta-task\n",
        "len(train.supports[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEd4rqTKqzKp"
      },
      "source": [
        "# Let take a look at the first two samples from support set\n",
        "train[0][0][:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nURtnKWJqzKp"
      },
      "source": [
        "## Training meta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwNGxw3PqzKp"
      },
      "source": [
        "import time\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.CRITICAL)\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "def random_seed(value):\n",
        "    torch.backends.cudnn.deterministic=True\n",
        "    torch.manual_seed(value)\n",
        "    torch.cuda.manual_seed(value)\n",
        "    np.random.seed(value)\n",
        "    random.seed(value)\n",
        "\n",
        "def create_batch_of_tasks(taskset, is_shuffle = True, batch_size = 4):\n",
        "    idxs = list(range(0,len(taskset)))\n",
        "    if is_shuffle:\n",
        "        random.shuffle(idxs)\n",
        "    for i in range(0,len(idxs), batch_size):\n",
        "        yield [taskset[idxs[i]] for i in range(i, min(i + batch_size,len(taskset)))]\n",
        "\n",
        "class TrainingArgs:\n",
        "    def __init__(self):\n",
        "        self.num_labels = 2\n",
        "        self.meta_epoch=2\n",
        "        self.k_spt=80\n",
        "        self.k_qry=20\n",
        "        self.outer_batch_size = 2\n",
        "        self.inner_batch_size = 12\n",
        "        self.outer_update_lr = 5e-5\n",
        "        self.inner_update_lr = 5e-5\n",
        "        self.inner_update_step = 10\n",
        "        self.inner_update_step_eval = 40\n",
        "        self.bert_model = 'microsoft/codebert-base'\n",
        "        self.num_task_train = 10\n",
        "        self.num_task_test = 5\n",
        "\n",
        "args = TrainingArgs()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGH1ckDTqzKq"
      },
      "source": [
        "## Create Meta Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb0GUvpKqzKq"
      },
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import BertForSequenceClassification, RobertaModel, RobertaForSequenceClassification\n",
        "from copy import deepcopy\n",
        "import gc\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Learner(nn.Module):\n",
        "    \"\"\"\n",
        "    Meta Learner\n",
        "    \"\"\"\n",
        "    def __init__(self, args):\n",
        "        \"\"\"\n",
        "        :param args:\n",
        "        \"\"\"\n",
        "        super(Learner, self).__init__()\n",
        "\n",
        "        self.num_labels = args.num_labels\n",
        "        self.outer_batch_size = args.outer_batch_size\n",
        "        self.inner_batch_size = args.inner_batch_size\n",
        "        self.outer_update_lr  = args.outer_update_lr\n",
        "        self.inner_update_lr  = args.inner_update_lr\n",
        "        self.inner_update_step = args.inner_update_step\n",
        "        self.inner_update_step_eval = args.inner_update_step_eval\n",
        "        self.bert_model = args.bert_model\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        #self.model = BertForSequenceClassification.from_pretrained(self.bert_model, num_labels = self.num_labels)\n",
        "        #self.model = AutoModelForSequenceClassification.from_pretrained('mrm8488/codebert-base-finetuned-detect-insecure-code')\n",
        "        self.model = RobertaForSequenceClassification.from_pretrained(\"microsoft/codebert-base\")\n",
        "        self.outer_optimizer = Adam(self.model.parameters(), lr=self.outer_update_lr)\n",
        "        self.model.train()\n",
        "\n",
        "    def forward(self, batch_tasks, training = True):\n",
        "        \"\"\"\n",
        "        batch = [(support TensorDataset, query TensorDataset),\n",
        "                 (support TensorDataset, query TensorDataset),\n",
        "                 (support TensorDataset, query TensorDataset),\n",
        "                 (support TensorDataset, query TensorDataset)]\n",
        "\n",
        "        # support = TensorDataset(all_input_ids, all_attention_mask, all_segment_ids, all_label_ids)\n",
        "        \"\"\"\n",
        "        task_accs = []\n",
        "        sum_gradients = []\n",
        "        num_task = len(batch_tasks)\n",
        "        num_inner_update_step = self.inner_update_step if training else self.inner_update_step_eval\n",
        "\n",
        "        for task_id, task in enumerate(batch_tasks):\n",
        "            support = task[0]\n",
        "            query   = task[1]\n",
        "\n",
        "            fast_model = deepcopy(self.model)\n",
        "            fast_model.to(self.device)\n",
        "            support_dataloader = DataLoader(support, sampler=RandomSampler(support),\n",
        "                                            batch_size=self.inner_batch_size)\n",
        "\n",
        "            inner_optimizer = Adam(fast_model.parameters(), lr=self.inner_update_lr)\n",
        "            fast_model.train()\n",
        "\n",
        "            print('----Task',task_id, '----')\n",
        "            for i in range(0,num_inner_update_step):\n",
        "                all_loss = []\n",
        "                for inner_step, batch in enumerate(support_dataloader):\n",
        "\n",
        "                    batch = tuple(t.to(self.device) for t in batch)\n",
        "                    input_ids, attention_mask, segment_ids, label_id = batch\n",
        "                    outputs = fast_model(input_ids, attention_mask, segment_ids, labels = label_id)\n",
        "\n",
        "                    loss = outputs[0]\n",
        "                    loss.backward()\n",
        "                    inner_optimizer.step()\n",
        "                    inner_optimizer.zero_grad()\n",
        "\n",
        "                    all_loss.append(loss.item())\n",
        "\n",
        "                if i % 4 == 0:\n",
        "                    print(\"Inner Loss: \", np.mean(all_loss))\n",
        "\n",
        "            fast_model.to(torch.device('cpu'))\n",
        "\n",
        "            if training:\n",
        "                meta_weights = list(self.model.parameters())\n",
        "                fast_weights = list(fast_model.parameters())\n",
        "\n",
        "                gradients = []\n",
        "                for i, (meta_params, fast_params) in enumerate(zip(meta_weights, fast_weights)):\n",
        "                    gradient = meta_params - fast_params\n",
        "                    if task_id == 0:\n",
        "                        sum_gradients.append(gradient)\n",
        "                    else:\n",
        "                        sum_gradients[i] += gradient\n",
        "\n",
        "            fast_model.to(self.device)\n",
        "            fast_model.eval()\n",
        "            with torch.no_grad():\n",
        "                query_dataloader = DataLoader(query, sampler=None, batch_size=len(query))\n",
        "                query_batch = next(iter(query_dataloader))\n",
        "                query_batch = tuple(t.to(self.device) for t in query_batch)\n",
        "                q_input_ids, q_attention_mask, q_segment_ids, q_label_id = query_batch\n",
        "                q_outputs = fast_model(q_input_ids, q_attention_mask, q_segment_ids, labels = q_label_id)\n",
        "\n",
        "                q_logits = F.softmax(q_outputs[1],dim=1)\n",
        "                pre_label_id = torch.argmax(q_logits,dim=1)\n",
        "                pre_label_id = pre_label_id.detach().cpu().numpy().tolist()\n",
        "                q_label_id = q_label_id.detach().cpu().numpy().tolist()\n",
        "\n",
        "                acc = accuracy_score(pre_label_id,q_label_id)\n",
        "                task_accs.append(acc)\n",
        "\n",
        "            fast_model.to(torch.device('cpu'))\n",
        "            del fast_model, inner_optimizer\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        if training:\n",
        "            # Average gradient across tasks\n",
        "            for i in range(0,len(sum_gradients)):\n",
        "                sum_gradients[i] = sum_gradients[i] / float(num_task)\n",
        "\n",
        "            #Assign gradient for original model, then using optimizer to update its weights\n",
        "            for i, params in enumerate(self.model.parameters()):\n",
        "                params.grad = sum_gradients[i]\n",
        "\n",
        "            self.outer_optimizer.step()\n",
        "            self.outer_optimizer.zero_grad()\n",
        "\n",
        "            del sum_gradients\n",
        "            gc.collect()\n",
        "\n",
        "        return np.mean(task_accs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNqBn6akqzKq"
      },
      "source": [
        "learner = Learner(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lg_DJBEoqzKr"
      },
      "source": [
        "random_seed(123)\n",
        "test = MetaTask(test_examples, num_task =1, k_support=30, k_query=10, tokenizer = tokenizer)\n",
        "random_seed(int(time.time() % 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJFZCvIbqzKs"
      },
      "source": [
        "## Start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ-T4tTJqzKs"
      },
      "source": [
        "global_step = 0\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "for epoch in range(args.meta_epoch):\n",
        "\n",
        "    train = MetaTask(train_examples, num_task = 5, k_support=80, k_query=20, tokenizer = tokenizer)\n",
        "    db = create_batch_of_tasks(train, is_shuffle = True, batch_size = args.outer_batch_size)\n",
        "\n",
        "    for step, task_batch in enumerate(db):\n",
        "        print(len(task_batch))\n",
        "        f = open('log.txt', 'a')\n",
        "\n",
        "        acc = learner(task_batch)\n",
        "\n",
        "        print('Step:', step, '\\ttraining Acc:', acc)\n",
        "        f.write(str(acc) + '\\n')\n",
        "\n",
        "        if global_step % 20 == 0:\n",
        "            random_seed(123)\n",
        "            print(\"\\n-----------------Testing Mode-----------------\\n\")\n",
        "            db_test = create_batch_of_tasks(test, is_shuffle = False, batch_size = 1)\n",
        "            acc_all_test = []\n",
        "\n",
        "            for test_batch in db_test:\n",
        "                acc = learner(test_batch, training = False)\n",
        "                acc_all_test.append(acc)\n",
        "\n",
        "            print('Step:', step, 'Test F1:', np.mean(acc_all_test))\n",
        "            f.write('Test' + str(np.mean(acc_all_test)) + '\\n')\n",
        "\n",
        "            random_seed(int(time.time() % 10))\n",
        "\n",
        "        global_step += 1\n",
        "        f.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}